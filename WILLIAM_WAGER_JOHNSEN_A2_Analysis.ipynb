{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## William W. Johnsen Analysis A2\n",
    "\n",
    "Starting off with importing packages, followed by setting print options for panda prints.\n",
    "\n",
    "Finally reading the file I am working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.model_selection import GridSearchCV     # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer              # customizable scorer                    # random number gen\n",
    "import pandas            as pd                       # data science essentials\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import seaborn           as sns                      # enhanced data viz\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# reading file\n",
    "original_df = pd.read_excel('Apprentice_Chef_Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature creation\n",
    "In the cell below I am converting the 'FOLLOWED_RECOMMENDATIONS_PCT' column into actual numbers, instead of keeping percentages. If the percentage is 0, the number will remain 0, or else the number in that cell will end up being infinite. \n",
    "\n",
    "After that I am creating a couple of features:\n",
    "\n",
    "- Average time each person use per click per visit. \n",
    "- Weekly orders.\n",
    "- If someone has attended one or more Master Classes.\n",
    "- If anybody does not have a last name, fill 'Unknown'.\n",
    "- If anybody have same family name as first name, replace last name with 'Unkown'. \n",
    "- Make a dummy for 'Unkown' family name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['RECOMMENDED_MEALS'] = 0 \n",
    "\n",
    "for index, val in original_df.iterrows():\n",
    "    if original_df.loc[index,'FOLLOWED_RECOMMENDATIONS_PCT'] > 0:\n",
    "        original_df.loc[index, 'RECOMMENDED_MEALS'] = original_df.loc[index, 'TOTAL_MEALS_ORDERED']*(original_df.loc[index,'FOLLOWED_RECOMMENDATIONS_PCT']/100)\n",
    "    elif original_df.loc[index,'FOLLOWED_RECOMMENDATIONS_PCT'] == 0:\n",
    "         original_df.loc[index, 'RECOMMENDED_MEALS'] = 0\n",
    "    else:\n",
    "        print(\"what\")\n",
    "        \n",
    "\n",
    "original_df['AVG_TIME_CLICK'] = original_df['AVG_TIME_PER_SITE_VISIT']/original_df['AVG_CLICKS_PER_VISIT']\n",
    "# original_df['NOT_ON_SCHEDULE_PCT'] = ((original_df['EARLY_DELIVERIES']+original_df['LATE_DELIVERIES'])/original_df['TOTAL_MEALS_ORDERED'])*100\n",
    "# original_df['CANCEL_PCT'] = ((original_df['CANCELLATIONS_BEFORE_NOON']+original_df['CANCELLATIONS_AFTER_NOON'])/original_df['TOTAL_MEALS_ORDERED'])*100\n",
    "original_df['Weekly_Orders'] = original_df['TOTAL_MEALS_ORDERED']/52\n",
    "# original_df['TOTAL_TIME'] = original_df['TOTAL_MEALS_ORDERED']*original_df['AVG_TIME_PER_SITE_VISIT']\n",
    "# original_df['TOTAL_CLICKS'] = original_df['TOTAL_MEALS_ORDERED']*original_df['AVG_CLICKS_PER_VISIT']\n",
    "\n",
    "original_df['MASTER_UP_1'] = 0\n",
    "for index, val in original_df.iterrows():\n",
    "    if original_df.loc[index, 'MASTER_CLASSES_ATTENDED'] >= 1:\n",
    "        original_df.loc[index, 'MASTER_UP_1'] = 1 \n",
    "        \n",
    "\n",
    "        \n",
    "# fill blank with unknown, assuming if the last and first is equal, they dont have a registered last name\n",
    "original_df['FAMILY_NAME'] = original_df['FAMILY_NAME'].fillna('Unknown')\n",
    "\n",
    "\n",
    "#if last name is equal first name, fill last with unknown\n",
    "for index, val in original_df.iterrows():\n",
    "    if original_df.loc[index, 'FIRST_NAME'] == original_df.loc[index, 'FAMILY_NAME']:\n",
    "        original_df.loc[index, 'FAMILY_NAME'] = 'Unknown' \n",
    "\n",
    "\n",
    "        \n",
    "original_df['UNKOWN'] = 0\n",
    "\n",
    "for row, col in original_df.iterrows():\n",
    "    if original_df.loc[row, 'FAMILY_NAME'] == 'Unknown':\n",
    "        original_df.loc[row ,'UNKOWN'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets hope the Targaryen are loyal people in real life, compared to the series. \n",
    "\n",
    "This is not part of the code anymore since they obviously are not. (Not significant for the model or prediction. \n",
    "\n",
    "Neither is the family with the most counts (The Freys). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, val in original_df.iterrows():\n",
    "    if original_df.loc[index,'FAMILY_NAME'].endswith('Targaryen'):\n",
    "        original_df.loc[index, 'FAMILY_NAME'] = 'Targaryen'\n",
    "    elif original_df.loc[index,'FAMILY_NAME'].endswith('Targaryen '):\n",
    "        original_df.loc[index, 'FAMILY_NAME'] = 'Targaryen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking emails that contains a parantheses\n",
    "\n",
    "The logic behind this is to mark if an email is actually getting to the client. If they have a special character like a parantheses, sending an email to them won't actually go through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if email has ( )  inside, this indicates that mail is invalid and will not reach client\n",
    "original_df['INVALID_EMAIL'] = 0\n",
    "\n",
    "for index, val in original_df.iterrows():\n",
    "    if '(' in original_df.loc[index, 'EMAIL']:\n",
    "        original_df.loc[index, 'INVALID_EMAIL'] = 1\n",
    "    elif ')' in original_df.loc[index, 'EMAIL']:\n",
    "        original_df.loc[index, 'INVALID_EMAIL'] = 1\n",
    "    else: \n",
    "        original_df.loc[index, 'INVALID_EMAIL'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot-encoding the emails\n",
    "\n",
    "Sorting the emails into:\n",
    "- Professional email domains.\n",
    "- Personal email domains.\n",
    "- Junk email domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting emails into professional, personal and junk\n",
    "\n",
    "# create a list to hold the different domains\n",
    "p_lst = []\n",
    "\n",
    "\n",
    "# looping over each email \n",
    "for i, col in original_df.iterrows():\n",
    "    \n",
    "    # splitting emails \n",
    "    split_m = original_df.loc[i, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # append to list\n",
    "    p_lst.append(split_m)\n",
    "\n",
    "    \n",
    "# convert list to dataframe, followed by renaming the column names\n",
    "email_df = pd.DataFrame(p_lst)\n",
    "\n",
    "email_df.columns = ['name', 'domain']\n",
    "\n",
    "\n",
    "# concatenating with original df\n",
    "original_df = pd.concat([original_df, email_df.loc[:, 'domain']], axis = 1)\n",
    "\n",
    "\n",
    "# defining the different group of domains.\n",
    "professional_email = ['@mmm.com', '@amex.com', '@apple.com',  '@boeing.com','@caterpillar.com',\n",
    "                      '@chevron.com','@cisco.com',  '@cocacola.com','@disney.com','@dupont.com',\n",
    "                      '@exxon.com', '@ge.org','@goldmansacs.com','@homedepot.com','@ibm.com',\n",
    "                      '@intel.com','@jnj.com','@jpmorgan.com','@mcdonalds.com','@merck.com',\n",
    "                      '@microsoft.com','@nike.com','@pfizer.com','@pg.com', '@travelers.com',\n",
    "                      '@unitedtech.com','@unitedhealth.com','@verizon.com', '@visa.com','@walmart.com']\n",
    "\n",
    "personal_email     = ['@gmail.com', '@yahoo.com', '@protonmail.com']\n",
    "\n",
    "junk_email         = ['@me.com', '@aol.com','@hotmail.com','@live.com','@msn.com','@passport.com']\n",
    "\n",
    "# creating a list to hold the count of domains\n",
    "p_lst              = []\n",
    "\n",
    "for address in original_df['domain']:\n",
    "    if '@' + address in professional_email:\n",
    "        p_lst.append('professional')\n",
    "        \n",
    "    elif '@' + address in personal_email:\n",
    "        p_lst.append('personal')\n",
    "        \n",
    "    elif '@' + address in junk_email:\n",
    "        p_lst.append('junk')\n",
    "    else:\n",
    "        print('Unknown happening.')\n",
    "\n",
    "# converting the list of domains into a new column in original_df\n",
    "original_df['domain_group'] = pd.Series(p_lst)\n",
    "\n",
    "\n",
    "# one hot encoding categorical data - here emails\n",
    "one_hot_Email = pd.get_dummies(original_df['domain_group'])\n",
    "\n",
    "# dropping categorical variables after they have been encoded\n",
    "original_df = original_df.drop(['domain_group','domain'], axis = 1)\n",
    "\n",
    "# joining codings together\n",
    "original_df = original_df.join([one_hot_Email])\n",
    "\n",
    "# saving new columns\n",
    "new_columns = original_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting names\n",
    "\n",
    "Here I  count the number of names that a person has, but I didn't manage to do it in the manner I wanted to.\n",
    "\n",
    "The idea was to calculate every name the person has, but if they had a title like \"son of ______\", everything inside parentheses should be counted as one. \n",
    "\n",
    "I did not manage to accomplish that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of names\n",
    "original_df['N_NAMES'] = 0\n",
    "for index, val in original_df.iterrows():\n",
    "            original_df.loc[index, 'N_NAMES'] = len(original_df.loc[index, 'NAME'].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping text variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = original_df.drop(labels= ['FAMILY_NAME','FOLLOWED_RECOMMENDATIONS_PCT','NAME', 'EMAIL', 'FIRST_NAME'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corr = original_df.corr().round(2)\n",
    "\n",
    "# df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = original_df.drop('CROSS_SELL_SUCCESS', axis = 1)\n",
    "original_target = original_df.loc[:, 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data\n",
    "\n",
    "Scaling data because some observations is within thousands, some hundred, some tens and some are between 0 to 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(original_data)\n",
    "\n",
    "X_scaled = scaler.transform(original_data)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "X_scaled_df.columns = original_data.columns\n",
    "\n",
    "# train-test split\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled_df,\n",
    "                                                        original_target,\n",
    "                                                        test_size = 0.25,\n",
    "                                                        random_state = 222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating logistic model \n",
    "\n",
    "I am skipping the step of finding the proper variables with significant p-values. \n",
    "\n",
    "\n",
    "\"In machine learning we usually don't look into these\" \n",
    "(Ref. Professor Chase Kusterer)\n",
    "\n",
    "A model might also find some information within unsignificant values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8012\n",
      "Testing  ACCURACY: 0.7947\n",
      "AUC Score        : 0.7422\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg_scale = LogisticRegression(solver = 'lbfgs',\n",
    "                            random_state = 222)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_scaled_fit = logreg_scale.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_scaled_pred = logreg_scaled_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "log_reg_scaled_train = logreg_scaled_fit.score(X_train_scaled, y_train_scaled).round(4)\n",
    "log_reg_scaled_test = logreg_scaled_fit.score(X_test_scaled, y_test_scaled).round(4)\n",
    "log_reg_scaled_AUC   = roc_auc_score(y_true  = y_test_scaled, y_score = logreg_scaled_pred).round(4)\n",
    "\n",
    "print('Training ACCURACY:', log_reg_scaled_train)\n",
    "print('Testing  ACCURACY:', log_reg_scaled_test)\n",
    "print('AUC Score        :', log_reg_scaled_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 94  65]\n",
      " [ 35 293]]\n"
     ]
    }
   ],
   "source": [
    "# creating a confusion matrix\n",
    "print(confusion_matrix(y_true = y_test_scaled,\n",
    "                       y_pred = logreg_scaled_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Running GridSearchCV on the LogisticRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# # GridSearchCV\n",
    "# ########################################\n",
    "\n",
    "# # declaring a hyperparameter space\n",
    "# C_space          = pd.np.arange(0.1, 3.0, 0.1)\n",
    "# warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'C'          : C_space,\n",
    "#               'warm_start' : warm_start_space}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# lr_tuned = LogisticRegression(solver = 'lbfgs',\n",
    "#                               max_iter=1000,\n",
    "#                               random_state = 222)\n",
    "\n",
    "\n",
    "# # GridSearchCV object\n",
    "# lr_tuned_cv = GridSearchCV(estimator  = lr_tuned,\n",
    "#                            param_grid = param_grid,\n",
    "#                            cv         = 3,\n",
    "#                            scoring    = make_scorer(roc_auc_score,\n",
    "#                                                     needs_threshold = False))\n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# lr_tuned_cv.fit(X_scaled_df, original_target)\n",
    "\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "# print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the GridSearchCV\n",
    "\n",
    "Results are worse than the untuned Logistic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7985\n",
      "Testing  ACCURACY: 0.7926\n",
      "AUC Score        : 0.7342\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=222, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=True)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "log_reg_scaled_train_tuned = logreg_fit.score(X_train_scaled, y_train_scaled).round(4)\n",
    "log_reg_scaled_test_tuned  = logreg_fit.score(X_test_scaled, y_test_scaled).round(4)\n",
    "log_reg_scaled_AUC_tuned   = roc_auc_score(y_true  = y_test_scaled, y_score = logreg_pred).round(4)\n",
    "\n",
    "print('Training ACCURACY:', log_reg_scaled_train_tuned)\n",
    "print('Testing  ACCURACY:', log_reg_scaled_test_tuned)\n",
    "print('AUC Score        :', log_reg_scaled_AUC_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90  69]\n",
      " [ 32 296]]\n"
     ]
    }
   ],
   "source": [
    "# creating a confusion matrix\n",
    "print(confusion_matrix(y_true = y_test_scaled,\n",
    "                       y_pred = logreg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and trial dumping\n",
    "\n",
    "This part is for models and feature engineering that turned out to be insignificant and/or bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON SCALED TRAIN_TEST_SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            original_data,\n",
    "            original_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 222,\n",
    "            stratify = original_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out_cancel_hi = 10\n",
    "# Out_not_on_sc_hi = 40\n",
    "# Out_avg_time_click_hi = 25\n",
    "# Out_rec_meals_hi = 100\n",
    "# Out_total_photos = 300\n",
    "# Out_avg_prep_hi = 300\n",
    "# Out_avg_time_visit_hi = 300\n",
    "# Out_total_meals = 300\n",
    "# Out_rev_hi = 4000\n",
    "\n",
    "\n",
    "# original_df['Out_Cancel'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_Cancel'][original_df['CANCEL_PCT'] > Out_cancel_hi] \n",
    "\n",
    "\n",
    "# original_df['Out_Cancel'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "# original_df['Out_Not_Schedule'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_Not_Schedule'][original_df['NOT_ON_SCHEDULE_PCT'] > Out_not_on_sc_hi] \n",
    "\n",
    "\n",
    "# original_df['Out_Not_Schedule'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "\n",
    "# original_df['Out_Avg_Time_Click'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_Avg_Time_Click'][original_df['AVG_TIME_CLICK'] > Out_avg_time_click_hi] \n",
    "\n",
    "\n",
    "# original_df['Out_Avg_Time_Click'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "# original_df['Out_Rec_Meals'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_Rec_Meals'][original_df['RECOMMENDED_MEALS'] > Out_rec_meals_hi] \n",
    "\n",
    "\n",
    "# original_df['Out_Rec_Meals'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# original_df['Out_TotPhoto'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_TotPhoto'][original_df['TOTAL_PHOTOS_VIEWED'] > Out_total_photos] \n",
    "\n",
    "\n",
    "# original_df['Out_TotPhoto'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "# original_df['Out_Avg_prep'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_Avg_prep'][original_df['AVG_PREP_VID_TIME'] > Out_avg_prep_hi] \n",
    "\n",
    "\n",
    "# original_df['Out_Avg_prep'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "# original_df['Out_Avg_Time_Visit'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_Avg_Time_Visit'][original_df['AVG_TIME_PER_SITE_VISIT'] > Out_avg_time_visit_hi] \n",
    "\n",
    "\n",
    "# original_df['Out_Avg_Time_Visit'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "# original_df['Out_TotMeals'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_TotMeals'][original_df['TOTAL_MEALS_ORDERED'] > Out_total_meals] \n",
    "\n",
    "\n",
    "# original_df['Out_TotMeals'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)\n",
    "\n",
    "# original_df['Out_Rev'] = 0\n",
    "\n",
    "# condition_hi = original_df.loc[0:, 'Out_Rev'][original_df['REVENUE'] > Out_rev_hi] \n",
    "\n",
    "\n",
    "# original_df['Out_Rev'].replace(to_replace = condition_hi,\n",
    "#                                 value   = 1,\n",
    "#                                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9013\n",
      "Testing ACCURACY : 0.7906\n",
      "AUC Score        : 0.736\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = 3,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 222)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_default_fit = full_gbm_default.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "full_scaled_gbm_train = full_gbm_default_fit.score(X_train_scaled, y_train_scaled).round(4)\n",
    "full_scaled_gbm_test = full_gbm_default_fit.score(X_test_scaled, y_test_scaled).round(4)\n",
    "full_scaled_gbm_AUC = roc_auc_score(y_true  = y_test_scaled,\n",
    "                                          y_score = full_gbm_default_pred).round(4)\n",
    "\n",
    "print('Training ACCURACY:', full_scaled_gbm_train)\n",
    "print('Testing ACCURACY :', full_scaled_gbm_test)\n",
    "print('AUC Score        :', full_scaled_gbm_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.669\n",
      "Testing ACCURACY : 0.6694\n",
      "AUC Score        : 0.7458\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_non = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = 3,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 222)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_non_fit = full_gbm_non.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_non_pred = full_gbm_non_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "full_non_gbm_train = full_gbm_default_fit.score(X_train, y_train_scaled).round(4)\n",
    "full_non_gbm_test = full_gbm_default_fit.score(X_test, y_test_scaled).round(4)\n",
    "full_non_gbm_AUC = roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_gbm_non_pred).round(4)\n",
    "\n",
    "print('Training ACCURACY:', full_non_gbm_train)\n",
    "print('Testing ACCURACY :', full_non_gbm_test)\n",
    "print('AUC Score        :', full_non_gbm_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9952\n",
      "Testing  ACCURACY: 0.7474\n",
      "AUC Score        : 0.6736\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(n_estimators     = 100,\n",
    "                                    criterion        = 'gini',\n",
    "                                    max_depth        = None,\n",
    "                                    min_samples_leaf = 2,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 222)\n",
    "\n",
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "rf_non_scaled_train = rf_default_fit.score(X_train, y_train).round(4)\n",
    "rf_non_scaled_test = rf_default_fit.score(X_test, y_test).round(4)\n",
    "rf_non_scaled_AUC = roc_auc_score(y_true  = y_test, y_score = rf_default_fit_pred).round(4)\n",
    "\n",
    "print('Training ACCURACY:', rf_non_scaled_train)\n",
    "print('Testing  ACCURACY:', rf_non_scaled_test)\n",
    "print('AUC Score        :', rf_non_scaled_AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out the final results. \n",
    "\n",
    "The logistic model which was using scaled numbers, but was not tuned with GridSearchCV was the strongest model I could come up with.\n",
    "\n",
    "To finalize this project I would consider this model a relatively weak one. I had a lot of struggles with creating features that would help me explain how the company could improve their chance of predicting which customers that would take advantage of the campaign \"Halfway There\". \n",
    "\n",
    "I am looking forward to the next class to get some insights of how the professor and other classmates have solved this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN Model                  Train Score       Test Score       AUC \n",
      "----------------           ----------       ----------      ----------\n",
      "Random Forest No Scale      0.9952             0.7474         0.6736\n",
      "GBM non Scaled              0.9013             0.7906         0.736\n",
      "GBM Scaled                  0.669              0.6694         0.7458\n",
      "Logistic Scaled             0.7985             0.7926         0.7342\n",
      "Logistic CV Scaled          0.8012             0.7947         0.7422\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comparing results\n",
    "\n",
    "print(f\"\"\"\n",
    "KNN Model                  Train Score       Test Score       AUC \n",
    "----------------           ----------       ----------      ----------\n",
    "Random Forest No Scale      {rf_non_scaled_train}             {rf_non_scaled_test}         {rf_non_scaled_AUC}\n",
    "GBM non Scaled              {full_scaled_gbm_train}             {full_scaled_gbm_test}         {full_scaled_gbm_AUC}\n",
    "GBM Scaled                  {full_non_gbm_train}              {full_non_gbm_test}         {full_non_gbm_AUC}\n",
    "Logistic CV Scaled             {log_reg_scaled_train_tuned}             {log_reg_scaled_test_tuned}         {log_reg_scaled_AUC_tuned}\n",
    "Logistic Scaled          {log_reg_scaled_train}             {log_reg_scaled_test}         {log_reg_scaled_AUC}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
